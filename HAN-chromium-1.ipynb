{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 23:48:04.279306: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 23:48:04.902739: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-08 23:48:04.902762: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-08 23:48:06.264550: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 23:48:06.264882: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-08 23:48:06.264893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"8086:5917\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import json, re, nltk, string\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Bidirectional,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    RepeatVector,\n",
    "    TimeDistributed,\n",
    "    Permute,\n",
    "    multiply,\n",
    "    Lambda,\n",
    "    Activation,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam # - Works\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bugs_json = './data/mozilla_firefox/all_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GLOVE\n",
    "# glove_file = './data/google_chromium/vectors.txt'\n",
    "# tmp_file = './data/google_chromium/glove.txt'\n",
    "# glove2word2vec(glove_file, tmp_file)\n",
    "# wordvec_model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "# vocabulary = wordvec_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec parameters\n",
    "min_word_frequency = 5\n",
    "embed_size = 200\n",
    "context_window = 5\n",
    "\n",
    "# NN hyperparameters\n",
    "num_cv = 10\n",
    "max_sentence_num = 20\n",
    "max_sentence_len = 10\n",
    "num_rnn_unit = 512\n",
    "num_dense_unit = 1000\n",
    "rank_k = 10\n",
    "batch_size = 256\n",
    "\n",
    "# Mozilla firefox repeated sentence\n",
    "removal_sent = ['Steps to Reproduce:',\n",
    "                'Expected Results:',\n",
    "                'Actual Results:',\n",
    "                'Builds Tested On:',\n",
    "                'Additional Information']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22136\n",
      "140171\n",
      "135880\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "with open(all_bugs_json) as data_file:\n",
    "    text = data_file.read()\n",
    "    text = text.replace('\" : NULL', '\" : \"NULL\"')\n",
    "    data = json.loads(text, strict=False)\n",
    "\n",
    "open_title = []\n",
    "open_desc = []\n",
    "closed_title = []\n",
    "closed_desc = []\n",
    "closed_owner = []\n",
    "for item in data:\n",
    "    # Firefox\n",
    "    status = ['VERIFIED', 'RESOLVED']\n",
    "    # bug_type = ['Bug', 'Bug-Security', 'Bug-Regression', 'Bug-PlayStoreReview', 'Bug-Bug', 'BugMTBC']\n",
    "    if item['status'] in status and item['owner']:\n",
    "        closed_title.append(item['issue_title'])\n",
    "        closed_desc.append(item['description'])\n",
    "        closed_owner.append(item['owner'])\n",
    "    else:\n",
    "        open_title.append(item['issue_title'])\n",
    "        open_desc.append(item['description'])\n",
    "\n",
    "closed_title_20 = []\n",
    "closed_desc_20 = []\n",
    "closed_owner_20 = []\n",
    "owner = {}\n",
    "for key in closed_owner:\n",
    "    owner[key] = owner.get(key, 0) + 1\n",
    "for i in range(len(closed_owner)):\n",
    "    if owner[closed_owner[i]] >= 20:\n",
    "        closed_title_20.append(closed_title[i])\n",
    "        closed_desc_20.append(closed_desc[i])\n",
    "        closed_owner_20.append(closed_owner[i])\n",
    "\n",
    "print(len(open_title))\n",
    "print(len(closed_title))\n",
    "print(len(closed_title_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nobody@mozilla.org', 96616)\n",
      "('bugzilla@blakeross.com', 11522)\n",
      "('bugs@bengoodger.com', 3411)\n",
      "('dao+bmo@mozilla.com', 1236)\n",
      "('p_ch@verizon.net', 1019)\n",
      "('gijskruitbosch+bugs@gmail.com', 800)\n",
      "('asaf@sent.com', 731)\n",
      "('vporof@mozilla.com', 710)\n",
      "('mak77@bonardo.net', 634)\n",
      "('gavin.sharp@gmail.com', 519)\n",
      "187\n"
     ]
    }
   ],
   "source": [
    "# Owner details\n",
    "owner_cnt = {}\n",
    "for owner in closed_owner_20:\n",
    "    owner_cnt[owner] = owner_cnt.get(owner, 0) + 1\n",
    "sorted_owner_cnt = sorted(owner_cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(10):\n",
    "    print(sorted_owner_cnt[i])\n",
    "print(len(sorted_owner_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess(title, desc):\n",
    "    # Remove \\r and repeated sentence\n",
    "    current_title = title.replace('\\r', ' ')\n",
    "    current_desc = desc.replace('\\r', ' ')\n",
    "    for sent in removal_sent:\n",
    "        current_desc = current_desc.replace(sent, ' ')\n",
    "    # Remove URLs\n",
    "    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)\n",
    "    # Change to lower case\n",
    "    current_title = current_title.lower()\n",
    "    current_desc = current_desc.lower()\n",
    "    # Remove stack trace\n",
    "    start_loc = current_desc.find(\"stack trace\")\n",
    "    current_desc = current_desc[:start_loc]    \n",
    "    # Remove hex code\n",
    "    current_title = re.sub(r'(\\w+)0x\\w+', '', current_title)\n",
    "    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n",
    "    # Tokenize sentence\n",
    "    current_title_tokens = nltk.sent_tokenize(current_title)\n",
    "    current_desc_tokens = nltk.sent_tokenize(current_desc)\n",
    "    current_desc_tokens_list = [desc.split('\\n') for desc in current_desc_tokens]\n",
    "    current_desc_tokens = []\n",
    "    for desc in current_desc_tokens_list:\n",
    "        current_desc_tokens += desc\n",
    "    # Remove punctuation\n",
    "    def remove_punct(report):\n",
    "        report_filter = []\n",
    "        for sent in report:\n",
    "            for punct in string.punctuation:\n",
    "                sent = sent.replace(punct, '')\n",
    "            report_filter.append(sent)\n",
    "        return report_filter\n",
    "    current_title_filter = remove_punct(current_title_tokens)\n",
    "    current_desc_filter = remove_punct(current_desc_tokens)\n",
    "    # Tokenize word\n",
    "    current_title_filter = [nltk.word_tokenize(sent) for sent in current_title_filter]\n",
    "    current_desc_filter = [nltk.word_tokenize(sent) for sent in current_desc_filter]\n",
    "    # Lemmatization\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    tagged_title = [nltk.pos_tag(title) for title in current_title_filter]\n",
    "    tagged_desc = [nltk.pos_tag(desc) for desc in current_desc_filter]\n",
    "    current_title_lemm = [[WordNetLemmatizer().lemmatize(tag[0], pos=get_wordnet_pos(tag[1]) or wordnet.NOUN) for tag in title] for title in tagged_title]\n",
    "    current_desc_lemm = [[WordNetLemmatizer().lemmatize(tag[0], pos=get_wordnet_pos(tag[1]) or wordnet.NOUN) for tag in desc] for desc in tagged_desc]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    current_title_stop = [[word for word in title if not word in stop_words] for title in current_title_lemm]\n",
    "    current_desc_stop = [[word for word in desc if not word in stop_words] for desc in current_desc_lemm]\n",
    "    # Merge title and description\n",
    "    current_report = current_title_stop + current_desc_stop\n",
    "    current_report = list(filter(None, current_report))\n",
    "    \n",
    "    return current_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "# Bug reports for pre-training word vectors\n",
    "open_report = []\n",
    "# open_word = {}\n",
    "for i in range(1000):\n",
    "    current_report = preprocess(open_title[i], open_desc[i])\n",
    "    # Flatten\n",
    "    current_report = [word for sent in current_report for word in sent]\n",
    "#     # Remove words appeared in more than 50% of reports\n",
    "#     unique_report = set(current_report)\n",
    "#     for word in unique_report:\n",
    "#         open_word[word] = open_word.get(word, 0) + 1\n",
    "#     for word in current_report:\n",
    "#         if open_word[word] >= len(open_title)//2:\n",
    "#             current_report.remove(word)\n",
    "    \n",
    "    open_report.append(current_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word vectors\n",
    "wordvec_model = Word2Vec(open_report, min_count=min_word_frequency, vector_size=embed_size, window=context_window)\n",
    "vocabulary = wordvec_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug reports for training and testing\n",
    "closed_report = []\n",
    "closed_owner = []\n",
    "for i in range(1000):\n",
    "    current_report = preprocess(closed_title_20[i], closed_desc_20[i])\n",
    "    closed_report.append(current_report)\n",
    "    closed_owner.append(closed_owner_20[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the words that is not present in the vocabulary\n",
    "update_report = []\n",
    "update_owner = []\n",
    "for i in range(len(closed_owner)):\n",
    "    update_sents = []\n",
    "    for sent in closed_report[i]:\n",
    "        current_sent = [word for word in sent if word in vocabulary]\n",
    "        update_sents.append(current_sent)\n",
    "    update_sents = list(filter(None, update_sents))\n",
    "    update_report.append(update_sents)\n",
    "    update_owner.append(closed_owner[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to numbers\n",
    "flatten_report = []\n",
    "for report in update_report:\n",
    "    for sent in report:\n",
    "        flatten_report.append(sent)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(flatten_report)\n",
    "\n",
    "for report in update_report:\n",
    "    for sent in report:\n",
    "        for i, word in enumerate(sent):\n",
    "            sent[i] = tokenizer.word_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = wordvec_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define topk_accuracy\n",
    "def topk_accuracy(prediction, y_test, classes, rank_k=10):\n",
    "    accuracy = []\n",
    "    sortedIndices = []\n",
    "    pred_classes = []\n",
    "    for ll in prediction:\n",
    "        sortedIndices.append(\n",
    "            sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True)\n",
    "        )\n",
    "    for k in range(1, rank_k + 1):\n",
    "        id = 0\n",
    "        trueNum = 0\n",
    "        for sortedInd in sortedIndices:\n",
    "            pred_classes.append(classes[sortedInd[:k]])\n",
    "            if np.argmax(y_test[id]) in sortedInd[:k]:\n",
    "                trueNum += 1\n",
    "            id += 1\n",
    "        accuracy.append((float(trueNum) / len(prediction)) * 100)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# # Define f_measure\n",
    "# def f_measure(prediction, y_test, classes, mode='macro'):\n",
    "#     y_pred = []\n",
    "#     y_true = []\n",
    "#     sortedIndices = []   \n",
    "#     for ll in prediction:\n",
    "#         sortedIndices.append(\n",
    "#             sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True)\n",
    "#         )\n",
    "#     id = 0\n",
    "#     for sortedInd in sortedIndices:\n",
    "#         ind = np.argmax(y_test[id])\n",
    "#         if ind in sortedInd[:10]:\n",
    "#             y_pred.append(ind)\n",
    "#         else:\n",
    "#             y_pred.append(-1)\n",
    "#         id += 1\n",
    "#     for y in y_test:\n",
    "#         y_true.append(np.argmax(y))\n",
    "            \n",
    "#     f1 = f1_score(y_true, y_pred, average = mode)\n",
    "    \n",
    "#     return f1\n",
    "\n",
    "# Class defining the custom attention layer\n",
    "class HierarchicalAttentionNetwork(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(HierarchicalAttentionNetwork, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim,)))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weight = [self.W, self.b, self.u]\n",
    "        super(HierarchicalAttentionNetwork, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "\n",
    "        ait = K.exp(K.squeeze(K.dot(uit, self.u), -1))\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        weighted_input = x * K.expand_dims(ait)\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "    def _get_attention_weights(self, X):\n",
    "\n",
    "        uit = K.tanh(K.bias_add(K.dot(X, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        return ait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 23:49:47.512652: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-08 23:49:47.512728: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-08 23:49:47.512761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NS13A2): /proc/driver/nvidia/version does not exist\n",
      "2023-02-08 23:49:47.513668: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/aryan/.local/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "CV3, top1 - ... - top10 accuracy:  [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV4, top1 - ... - top10 accuracy:  [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV2, top1 - ... - top10 accuracy:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV6, top1 - ... - top10 accuracy:  [0.0, 57.14285714285714, 71.42857142857143, 71.42857142857143, 71.42857142857143, 71.42857142857143, 71.42857142857143, 71.42857142857143, 71.42857142857143, 71.42857142857143]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f560c400160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f560c400160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV4, top1 - ... - top10 accuracy:  [60.0, 60.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f55dbf85240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f55dbf85240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV7, top1 - ... - top10 accuracy:  [75.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV5, top1 - ... - top10 accuracy:  [16.666666666666664, 33.33333333333333, 33.33333333333333, 33.33333333333333, 83.33333333333334, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV7, top1 - ... - top10 accuracy:  [12.5, 37.5, 62.5, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "CV6, top1 - ... - top10 accuracy:  [14.285714285714285, 42.857142857142854, 57.14285714285714, 85.71428571428571, 85.71428571428571, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "CV7, top1 - ... - top10 accuracy:  [0.0, 50.0, 75.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Top10 accuracies for all CVs: [100.0, 100.0, 100.0, 71.42857142857143, 100.0, 100.0, 100.0, 75.0, 100.0, 100.0]\n",
      "Average top10 accuracy: 94.64285714285714\n"
     ]
    }
   ],
   "source": [
    "# Train and test\n",
    "splitLength = len(update_report) // (num_cv + 1)\n",
    "slice_results = {}\n",
    "top_rank_k_accuracies = []\n",
    "# f1_measure = []\n",
    "for i in range(1, num_cv + 1):\n",
    "    print(i)\n",
    "    train_report = update_report[:i*splitLength-1]\n",
    "    train_owner = update_owner[:i*splitLength-1]\n",
    "    test_report = update_report[i*splitLength:(i+1)*splitLength-1]\n",
    "    test_owner = update_owner[i*splitLength:(i+1)*splitLength-1]\n",
    "        \n",
    "    # Remove data from test set that is not there in train set\n",
    "    train_owner_unique = set(train_owner)\n",
    "    test_owner_unique = set(test_owner)\n",
    "    unwanted_owner = list(test_owner_unique - train_owner_unique)\n",
    "    update_test_report = []\n",
    "    update_test_owner = []\n",
    "    for i in range(len(test_owner)):\n",
    "        if test_owner[i] not in unwanted_owner:\n",
    "            update_test_report.append(test_report[i])\n",
    "            update_test_owner.append(test_owner[i])\n",
    "    \n",
    "    unique_train_owner = list(set(train_owner))\n",
    "    classes = np.array(unique_train_owner)\n",
    "    \n",
    "    # Create train and test data\n",
    "    X_train = np.zeros(shape=[len(train_report), max_sentence_num, max_sentence_len], dtype=\"int32\")\n",
    "    Y_train = np.zeros(shape=[len(train_owner), 1], dtype=\"int32\")\n",
    "    for i, report in enumerate(train_report):\n",
    "        for j, sent in enumerate(report):\n",
    "            if j < max_sentence_num:\n",
    "                k = 0\n",
    "                for word in sent:\n",
    "                    if k < max_sentence_len:\n",
    "                        X_train[i, j, k] = word\n",
    "                        k = k + 1\n",
    "        Y_train[i, 0] = unique_train_owner.index(train_owner[i])\n",
    "    \n",
    "    X_test = np.zeros(shape=[len(update_test_report), max_sentence_num, max_sentence_len], dtype=\"int32\")\n",
    "    Y_test = np.zeros(shape=[len(update_test_owner), 1], dtype=\"int32\")\n",
    "    for i, report in enumerate(update_test_report):\n",
    "        for j, sent in enumerate(report):\n",
    "            if j < max_sentence_num:\n",
    "                k = 0\n",
    "                for word in sent:\n",
    "                    if k < max_sentence_len:\n",
    "                        X_test[i, j, k] = word\n",
    "                        k = k + 1\n",
    "        Y_test[i, 0] = unique_train_owner.index(update_test_owner[i])    \n",
    "    \n",
    "    y_train = np_utils.to_categorical(Y_train, len(unique_train_owner))\n",
    "    y_test = np_utils.to_categorical(Y_test, len(unique_train_owner))\n",
    "    \n",
    "    # Model\n",
    "    word_input = Input(shape=(max_sentence_len,), dtype='float32')\n",
    "    embedded_sequences = Embedding(len(embedding_matrix), embed_size, weights=[embedding_matrix], input_length=max_sentence_len, trainable=True)(word_input)\n",
    "    l_gru = Bidirectional(GRU(num_rnn_unit, return_sequences=True, dropout=0.2))(embedded_sequences)\n",
    "    l_dense = TimeDistributed(Dense(num_dense_unit))(l_gru)\n",
    "    l_att = HierarchicalAttentionNetwork(max_sentence_num)(l_dense)\n",
    "    word_encoder = Model(word_input, l_att)\n",
    "    \n",
    "    sent_input = Input(shape=(max_sentence_num, max_sentence_len), dtype='float32')\n",
    "    sent_encoder = TimeDistributed(word_encoder)(sent_input)\n",
    "    l_gru_sent = Bidirectional(GRU(num_rnn_unit, return_sequences=True, dropout=0.2))(sent_encoder)\n",
    "    l_dense_sent = TimeDistributed(Dense(num_dense_unit))(l_gru_sent)\n",
    "    l_att_sent = HierarchicalAttentionNetwork(max_sentence_len)(l_dense_sent)\n",
    "    preds = Dense(len(classes), activation='softmax')(l_att_sent)\n",
    "    model = Model(sent_input, preds)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=Adam(lr=1e-4), metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "    hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=500, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    accuracy = topk_accuracy(prediction, y_test, classes, rank_k=rank_k)\n",
    "#     f1 = f_measure(prediction, y_test, classes, mode='macro')\n",
    "    print(\"CV{0}, top1 - ... - top{1} accuracy: \".format(i, rank_k), accuracy)\n",
    "    \n",
    "    train_result = hist.history\n",
    "    train_result[\"test_topk_accuracies\"] = accuracy\n",
    "    slice_results[i + 1] = train_result\n",
    "    top_rank_k_accuracies.append(accuracy[-1])\n",
    "#     f1_measure.append(f1)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "print(\"Top{0} accuracies for all CVs: {1}\".format(rank_k, top_rank_k_accuracies))\n",
    "print(\"Average top{0} accuracy: {1}\".format(rank_k, sum(top_rank_k_accuracies)/rank_k))\n",
    "# print(f1_measure)\n",
    "# print(np.mean(f1_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
